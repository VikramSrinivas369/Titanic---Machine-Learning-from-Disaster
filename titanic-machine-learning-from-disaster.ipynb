{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set styling for matplotlib\nplt.style.use('fivethirtyeight')\nsns.set_palette(\"deep\")\nsns.set_style(\"whitegrid\")\n\n# Load the datasets\nprint(\"Loading and preparing data...\")\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# First: apply basic feature engineering to make sure 'Title' is created\ndef extract_title(df):\n    # Extract titles from names\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n    # Consolidate rare titles\n    title_mapping = {\n        'Mr': 'Mr',\n        'Miss': 'Miss',\n        'Mrs': 'Mrs',\n        'Master': 'Master',\n        'Dr': 'Officer',\n        'Rev': 'Officer',\n        'Col': 'Officer',\n        'Major': 'Officer',\n        'Mlle': 'Miss',\n        'Mme': 'Mrs',\n        'Don': 'Royalty',\n        'Sir': 'Royalty',\n        'Lady': 'Royalty',\n        'Countess': 'Royalty',\n        'Jonkheer': 'Royalty',\n        'Capt': 'Officer',\n        'Ms': 'Miss',\n        'Dona': 'Royalty'\n    }\n    df['Title'] = df['Title'].map(title_mapping)\n    return df\n\n# Apply title extraction to both datasets\ntrain_df = extract_title(train_df)\ntest_df = extract_title(test_df)\n\n# Function for comprehensive EDA visualizations\ndef visualize_titanic_data(df):\n    # Create a figure with subplots\n    fig = plt.figure(figsize=(20, 16))\n    gs = gridspec.GridSpec(3, 3)\n    \n    # 1. Survival by Gender\n    ax1 = plt.subplot(gs[0, 0])\n    survival_by_gender = df.groupby(['Sex', 'Survived']).size().unstack()\n    survival_by_gender_pct = survival_by_gender.div(survival_by_gender.sum(axis=1), axis=0) * 100\n    survival_by_gender_pct.plot(kind='bar', stacked=True, ax=ax1, color=['#ff6b6b', '#4ecdc4'])\n    ax1.set_title('Survival Rate by Gender', fontsize=14, fontweight='bold')\n    ax1.set_ylabel('Percentage (%)', fontweight='bold')\n    ax1.set_xticklabels(['Female', 'Male'], rotation=0)\n    \n    for p in ax1.patches:\n        width, height = p.get_width(), p.get_height()\n        if height > 5:  # Only show percentage for visible segments\n            x, y = p.get_xy() \n            ax1.text(x + width/2, y + height/2, f'{height:.1f}%', ha='center', va='center')\n    \n    # 2. Survival by Class\n    ax2 = plt.subplot(gs[0, 1])\n    survival_by_class = df.groupby(['Pclass', 'Survived']).size().unstack()\n    survival_by_class_pct = survival_by_class.div(survival_by_class.sum(axis=1), axis=0) * 100\n    survival_by_class_pct.plot(kind='bar', stacked=True, ax=ax2, color=['#ff6b6b', '#4ecdc4'])\n    ax2.set_title('Survival Rate by Class', fontsize=14, fontweight='bold')\n    ax2.set_ylabel('Percentage (%)', fontweight='bold')\n    ax2.set_xticklabels(['1st Class', '2nd Class', '3rd Class'], rotation=0)\n    \n    for p in ax2.patches:\n        width, height = p.get_width(), p.get_height()\n        if height > 5:\n            x, y = p.get_xy() \n            ax2.text(x + width/2, y + height/2, f'{height:.1f}%', ha='center', va='center')\n    \n    # 3. Age Distribution by Survival\n    ax3 = plt.subplot(gs[0, 2])\n    sns.kdeplot(data=df, x='Age', hue='Survived', fill=True, common_norm=False, alpha=0.7, \n                palette=['#ff6b6b', '#4ecdc4'], ax=ax3)\n    ax3.set_title('Age Distribution by Survival', fontsize=14, fontweight='bold')\n    ax3.set_xlabel('Age', fontweight='bold')\n    ax3.set_ylabel('Density', fontweight='bold')\n    \n    # 4. Fare Distribution by Survival\n    ax4 = plt.subplot(gs[1, 0])\n    df_fare = df.copy()\n    df_fare['Fare'] = df_fare['Fare'].clip(upper=df_fare['Fare'].quantile(0.99))  # Remove outliers\n    sns.boxplot(x='Pclass', y='Fare', hue='Survived', data=df_fare, palette=['#ff6b6b', '#4ecdc4'], ax=ax4)\n    ax4.set_title('Fare Distribution by Class and Survival', fontsize=14, fontweight='bold')\n    ax4.set_xlabel('Passenger Class', fontweight='bold')\n    ax4.set_ylabel('Fare (Â£)', fontweight='bold')\n    \n    # 5. Family Size vs Survival\n    ax5 = plt.subplot(gs[1, 1])\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    family_survival = df.groupby(['FamilySize', 'Survived']).size().unstack().fillna(0)\n    family_survival_rate = family_survival[1] / (family_survival[0] + family_survival[1]) * 100\n    \n    # Bar chart for counts\n    bars = ax5.bar(family_survival.index, family_survival.sum(axis=1), color='#b8e994')\n    ax5.set_xlabel('Family Size', fontweight='bold')\n    ax5.set_ylabel('Count', fontweight='bold', color='#6a8d73')\n    ax5.yaxis.set_major_locator(MaxNLocator(integer=True))\n    \n    # Line chart for survival rate\n    ax5_twin = ax5.twinx()\n    line = ax5_twin.plot(family_survival.index, family_survival_rate, 'o-', color='#ff6b6b', linewidth=3, markersize=8)\n    ax5_twin.set_ylabel('Survival Rate (%)', fontweight='bold', color='#ff6b6b')\n    ax5_twin.grid(False)\n    \n    ax5.set_title('Family Size: Count vs Survival Rate', fontsize=14, fontweight='bold')\n    \n    # 6. Embarked vs Survival\n    ax6 = plt.subplot(gs[1, 2])\n    df['Embarked'] = df['Embarked'].fillna('S')  # Fill missing values for visualization\n    embarked_survival = df.groupby(['Embarked', 'Survived']).size().unstack().fillna(0)\n    embarked_survival_pct = embarked_survival.div(embarked_survival.sum(axis=1), axis=0) * 100\n    embarked_survival_pct.plot(kind='bar', stacked=True, ax=ax6, color=['#ff6b6b', '#4ecdc4'])\n    \n    # Map port codes to names\n    port_names = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}\n    ax6.set_xticklabels([port_names.get(i, i) for i in embarked_survival.index], rotation=0)\n    ax6.set_title('Survival Rate by Port of Embarkation', fontsize=14, fontweight='bold')\n    ax6.set_ylabel('Percentage (%)', fontweight='bold')\n    \n    for p in ax6.patches:\n        width, height = p.get_width(), p.get_height()\n        if height > 5:\n            x, y = p.get_xy() \n            ax6.text(x + width/2, y + height/2, f'{height:.1f}%', ha='center', va='center')\n    \n    # 7. Title vs Survival\n    ax7 = plt.subplot(gs[2, 0:2])\n    title_survival = df.groupby(['Title', 'Survived']).size().unstack().fillna(0)\n    # Sort by survival rate\n    title_survival['SurvivalRate'] = title_survival[1] / (title_survival[0] + title_survival[1])\n    title_survival = title_survival.sort_values('SurvivalRate', ascending=False)\n    title_survival = title_survival.drop('SurvivalRate', axis=1)\n    \n    title_survival_pct = title_survival.div(title_survival.sum(axis=1), axis=0) * 100\n    title_survival_pct.plot(kind='barh', stacked=True, ax=ax7, color=['#ff6b6b', '#4ecdc4'], width=0.7)\n    ax7.set_title('Survival Rate by Title', fontsize=14, fontweight='bold')\n    ax7.set_xlabel('Percentage (%)', fontweight='bold')\n    \n    for p in ax7.patches:\n        width, height = p.get_width(), p.get_height()\n        if width > 5:\n            x, y = p.get_xy() \n            ax7.text(x + width/2, y + height/2, f'{width:.1f}%', ha='center', va='center')\n    \n    # 8. Age-Class Heatmap for Survival Rate\n    ax8 = plt.subplot(gs[2, 2])\n    # Create age bins for better visualization\n    df['AgeBin'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], \n                           labels=['Child', 'Teen', 'Young Adult', 'Adult', 'Senior'])\n    age_class_survival = pd.crosstab(index=[df['AgeBin']], \n                                     columns=[df['Pclass'], df['Survived']], \n                                     values=df['PassengerId'], \n                                     aggfunc='count').fillna(0)\n    \n    # Calculate survival rate\n    survival_matrix = np.zeros((len(age_class_survival), 3))\n    for i in range(len(age_class_survival)):\n        for j in range(3):\n            pclass = j + 1\n            if pclass in age_class_survival.columns.levels[0]:\n                if 1 in age_class_survival.columns.levels[1] and 0 in age_class_survival.columns.levels[1]:\n                    # Make sure both survived=0 and survived=1 exist\n                    survived = age_class_survival.iloc[i].get((pclass, 1), 0)\n                    total = survived + age_class_survival.iloc[i].get((pclass, 0), 0)\n                    survival_matrix[i, j] = survived / total * 100 if total > 0 else 0\n    \n    # Plot heatmap\n    sns.heatmap(survival_matrix, annot=True, fmt='.1f', cmap='RdYlGn', \n                xticklabels=['1st Class', '2nd Class', '3rd Class'],\n                yticklabels=age_class_survival.index, ax=ax8)\n    ax8.set_title('Survival Rate (%) by Age and Class', fontsize=14, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.subplots_adjust(top=0.95)\n    fig.suptitle('Titanic Survival Analysis', fontsize=20, fontweight='bold', y=0.98)\n    plt.show()\n\n# Function for interactive Plotly visualizations\ndef create_interactive_visualizations(df):\n    # 1. Interactive Survival by Age and Class\n    fig1 = px.scatter(df, x='Age', y='Fare', color='Survived', \n                     size='FamilySize', facet_col='Pclass', \n                     hover_name='Name', hover_data=['Sex', 'Ticket', 'Cabin', 'Embarked'],\n                     title='Survival by Age, Fare, Class and Family Size',\n                     color_continuous_scale=['#ff6b6b', '#4ecdc4'],\n                     labels={'Survived': 'Survived', 'Pclass': 'Passenger Class'},\n                     height=600)\n    fig1.update_layout(title_font_size=20)\n    fig1.show()\n    \n    # 2. Interactive Survival Correlation Heatmap\n    correlation_data = df[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', \n                          'Fare', 'Sex_encoded', 'Embarked_encoded', 'FamilySize']].corr()\n    \n    fig2 = go.Figure(data=go.Heatmap(\n                    z=correlation_data.values,\n                    x=correlation_data.columns,\n                    y=correlation_data.columns,\n                    colorscale='RdBu_r',\n                    zmin=-1, zmax=1,\n                    text=np.round(correlation_data.values, 2),\n                    texttemplate=\"%{text}\",\n                    textfont={\"size\":10}))\n    \n    fig2.update_layout(title='Feature Correlation Heatmap',\n                      title_font_size=20,\n                      height=600)\n    fig2.show()\n\n# Enhanced Feature Engineering\ndef enhance_features(df):\n    # Make a copy to avoid modifying the original\n    df = df.copy()\n    \n    # Extract titles from names (already done in the preprocessing step, but kept here for completeness)\n    if 'Title' not in df.columns:\n        df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n        \n        # Consolidate rare titles\n        title_mapping = {\n            'Mr': 'Mr',\n            'Miss': 'Miss',\n            'Mrs': 'Mrs',\n            'Master': 'Master',\n            'Dr': 'Officer',\n            'Rev': 'Officer',\n            'Col': 'Officer',\n            'Major': 'Officer',\n            'Mlle': 'Miss',\n            'Mme': 'Mrs',\n            'Don': 'Royalty',\n            'Sir': 'Royalty',\n            'Lady': 'Royalty',\n            'Countess': 'Royalty',\n            'Jonkheer': 'Royalty',\n            'Capt': 'Officer',\n            'Ms': 'Miss',\n            'Dona': 'Royalty'\n        }\n        df['Title'] = df['Title'].map(title_mapping)\n    \n    # Family size and type\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n    df['FamilyType'] = pd.cut(df['FamilySize'], bins=[0, 1, 4, 20], labels=['Solo', 'Small', 'Large'])\n    \n    # Age bands\n    df['AgeBand'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], \n                           labels=['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior'])\n    \n    # Extract cabin deck\n    df['Deck'] = df['Cabin'].str.slice(0, 1)\n    df['Deck'] = df['Deck'].fillna('U')  # U for unknown\n    \n    # Fare bands\n    df['FareBand'] = pd.qcut(df['Fare'].fillna(df['Fare'].median()), 5, \n                            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n    \n    # Combine Age and Class\n    df['Age*Class'] = df['Age'].fillna(df['Age'].median()) * df['Pclass']\n    \n    # Extract ticket prefix\n    df['TicketPrefix'] = df['Ticket'].str.extract('([A-Za-z]+)', expand=False)\n    df['TicketPrefix'] = df['TicketPrefix'].fillna('NUM')\n    \n    # Extract ticket length which may indicate social status\n    df['TicketLength'] = df['Ticket'].apply(lambda x: len(str(x)))\n    \n    # Create surname feature\n    df['Surname'] = df['Name'].str.split(',').str[0]\n    \n    # Interaction terms\n    df['FamilySizeClass'] = df['FamilySize'] * df['Pclass']\n    \n    return df\n\n# Improved data preprocessing function\ndef preprocess_data(train_df, test_df):\n    # Apply feature engineering\n    train_processed = enhance_features(train_df)\n    test_processed = enhance_features(test_df)\n    \n    # Impute missing ages with KNN\n    age_features = ['Pclass', 'SibSp', 'Parch', 'Fare']\n    knn_imputer = KNNImputer(n_neighbors=5)\n    \n    # Fit imputer on the training set\n    train_age_data = train_processed[['Age'] + age_features].copy()\n    train_processed['Age'] = knn_imputer.fit_transform(train_age_data)[:, 0]\n    \n    # Apply the same imputer to the test set\n    test_age_data = test_processed[['Age'] + age_features].copy()\n    test_processed['Age'] = knn_imputer.transform(test_age_data)[:, 0]\n    \n    # Fill other missing values\n    train_processed['Embarked'] = train_processed['Embarked'].fillna(train_processed['Embarked'].mode()[0])\n    test_processed['Fare'] = test_processed['Fare'].fillna(test_processed['Fare'].median())\n    \n    # Encode categorical features\n    for col in ['Sex', 'Embarked', 'Title', 'Deck', 'TicketPrefix', 'FamilyType', 'AgeBand', 'FareBand']:\n        if col in train_processed.columns:\n            # Create a combined encoder to ensure consistent encoding for both datasets\n            combined = pd.concat([train_processed[col].astype(str), test_processed[col].astype(str)])\n            encoder = LabelEncoder().fit(combined)\n            \n            # Apply to both datasets\n            train_processed[f'{col}_encoded'] = encoder.transform(train_processed[col].astype(str))\n            test_processed[f'{col}_encoded'] = encoder.transform(test_processed[col].astype(str))\n    \n    # Add survival features (for train data only)\n    if 'Survived' in train_df.columns:\n        # Add survival rate by title\n        title_survival_rate = train_processed.groupby('Title')['Survived'].mean()\n        train_processed['TitleSurvivalRate'] = train_processed['Title'].map(title_survival_rate)\n        test_processed['TitleSurvivalRate'] = test_processed['Title'].map(title_survival_rate)\n        \n        # Add survival rate by deck\n        deck_survival_rate = train_processed.groupby('Deck')['Survived'].mean()\n        train_processed['DeckSurvivalRate'] = train_processed['Deck'].map(deck_survival_rate)\n        test_processed['DeckSurvivalRate'] = test_processed['Deck'].map(deck_survival_rate)\n        \n        # Add family survival rate\n        family_survival = train_processed.groupby('Surname')['Survived'].mean()\n        train_processed['FamilySurvivalRate'] = train_processed['Surname'].map(family_survival)\n        test_processed['FamilySurvivalRate'] = test_processed['Surname'].map(family_survival)\n        \n        # Fill NaN survival rates\n        for col in ['TitleSurvivalRate', 'DeckSurvivalRate', 'FamilySurvivalRate']:\n            mean_rate = train_processed[col].mean()\n            train_processed[col] = train_processed[col].fillna(mean_rate)\n            test_processed[col] = test_processed[col].fillna(mean_rate)\n    \n    return train_processed, test_processed\n\n# Function for feature selection\ndef select_features(df, importance_threshold=0.01):\n    # Base features that are likely important\n    base_features = ['Pclass', 'Sex_encoded', 'Age', 'Fare', 'Embarked_encoded', \n                    'Title_encoded', 'FamilySize', 'IsAlone', 'TitleSurvivalRate', \n                    'DeckSurvivalRate', 'FamilySurvivalRate']\n    \n    # Add engineered features\n    engineered_features = ['Age*Class', 'FamilySizeClass', 'Deck_encoded', \n                          'AgeBand_encoded', 'FareBand_encoded']\n    \n    # Start with base features\n    selected_features = base_features + engineered_features\n    \n    # Remove features that don't exist in the dataframe\n    selected_features = [f for f in selected_features if f in df.columns]\n    \n    return selected_features\n\n# Function to train and evaluate multiple models\ndef train_evaluate_models(X_train, y_train, X_val, y_val):\n    models = {\n        'Logistic Regression': LogisticRegression(max_iter=1000, C=0.1),\n        'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42),\n        'SVM': SVC(probability=True, random_state=42)\n    }\n    \n    results = {}\n    feature_importances = None\n    \n    plt.figure(figsize=(15, 10))\n    \n    for i, (name, model) in enumerate(models.items()):\n        # Train model\n        model.fit(X_train, y_train)\n        \n        # Get predictions\n        y_pred = model.predict(X_val)\n        y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n        \n        # Calculate metrics\n        accuracy = accuracy_score(y_val, y_pred)\n        report = classification_report(y_val, y_pred, output_dict=True)\n        cm = confusion_matrix(y_val, y_pred)\n        \n        # Add to results\n        results[name] = {\n            'accuracy': accuracy,\n            'precision': report['1']['precision'],\n            'recall': report['1']['recall'],\n            'f1': report['1']['f1-score']\n        }\n        \n        # Confusion Matrix\n        plt.subplot(2, 2, i+1)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                   xticklabels=['Not Survived', 'Survived'],\n                   yticklabels=['Not Survived', 'Survived'])\n        plt.title(f'{name} (Accuracy: {accuracy:.3f})', fontsize=12, fontweight='bold')\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        \n        # Store feature importances for Random Forest\n        if name == 'Random Forest' and hasattr(model, 'feature_importances_'):\n            # Get column names (fixed)\n            feature_names = X_train.columns if hasattr(X_train, 'columns') else None\n            if feature_names is not None:\n                feature_importances = pd.Series(model.feature_importances_, index=feature_names)\n    \n    plt.tight_layout()\n    plt.suptitle('Confusion Matrices for Different Models', fontsize=16, fontweight='bold', y=1.02)\n    plt.show()\n    \n    # Plot feature importances for Random Forest\n    if feature_importances is not None:\n        plt.figure(figsize=(12, 8))\n        feature_importances.sort_values(ascending=False).plot(kind='bar')\n        plt.title('Feature Importances from Random Forest', fontsize=14, fontweight='bold')\n        plt.ylabel('Importance')\n        plt.tight_layout()\n        plt.show()\n    \n    # Plot comparative metrics\n    metrics = ['accuracy', 'precision', 'recall', 'f1']\n    comp_data = pd.DataFrame({model_name: [results[model_name][metric] for metric in metrics] \n                             for model_name in models.keys()}, index=metrics)\n    \n    plt.figure(figsize=(12, 6))\n    comp_data.plot(kind='bar')\n    plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n    plt.ylabel('Score')\n    plt.ylim(0, 1)\n    plt.xticks(rotation=0)\n    plt.legend(title='Models')\n    plt.tight_layout()\n    plt.show()\n    \n    return results, models\n\n# Function to create ensemble model\ndef create_ensemble(models):\n    print(\"Creating ensemble model...\")\n    estimators = [(name, model) for name, model in models.items()]\n    ensemble = VotingClassifier(estimators=estimators, voting='soft')\n    return ensemble\n\n# ROC curve and precision-recall curve\ndef plot_model_curves(models, X_val, y_val):\n    plt.figure(figsize=(15, 6))\n    \n    # ROC curve\n    plt.subplot(1, 2, 1)\n    for name, model in models.items():\n        if hasattr(model, 'predict_proba'):\n            y_pred_proba = model.predict_proba(X_val)[:, 1]\n            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves', fontsize=14, fontweight='bold')\n    plt.legend(loc='lower right')\n    \n    # Precision-Recall curve\n    plt.subplot(1, 2, 2)\n    for name, model in models.items():\n        if hasattr(model, 'predict_proba'):\n            y_pred_proba = model.predict_proba(X_val)[:, 1]\n            precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n            plt.plot(recall, precision, label=f'{name}')\n    \n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n    plt.legend(loc='best')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main execution flow\nprint(\"Starting Titanic survival analysis...\")\n\n# Run EDA visualizations on the training data with basic feature engineering\nprint(\"Generating EDA visualizations...\")\ntrain_with_features = enhance_features(train_df)\ntrain_with_features['Sex_encoded'] = LabelEncoder().fit_transform(train_with_features['Sex'])\ntrain_with_features['Embarked_encoded'] = LabelEncoder().fit_transform(train_with_features['Embarked'].fillna('S'))\nvisualize_titanic_data(train_with_features)\n\n# Interactive visualizations\nprint(\"Creating interactive visualizations...\")\ncreate_interactive_visualizations(train_with_features)\n\n# Preprocess data\nprint(\"Preprocessing data...\")\ntrain_processed, test_processed = preprocess_data(train_df, test_df)\n\n# Select features\nfeatures = select_features(train_processed)\nprint(f\"Selected {len(features)} features: {', '.join(features)}\")\n\n# Prepare training data\nX = train_processed[features]\ny = train_df['Survived']\nX_test = test_processed[features]\n\n# Split data for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale data - Modified to preserve DataFrame structure\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n\n# Train models\nprint(\"Training multiple models...\")\nresults, models = train_evaluate_models(X_train_scaled, y_train, X_val_scaled, y_val)\n\n# Plot model performance curves\nprint(\"Plotting model performance curves...\")\nplot_model_curves(models, X_val_scaled, y_val)\n\n# Create and train ensemble model\nensemble = create_ensemble(models)\nensemble.fit(X_train_scaled, y_train)\nensemble_pred = ensemble.predict(X_val_scaled)\nensemble_accuracy = accuracy_score(y_val, ensemble_pred)\nprint(f'Ensemble Model Validation Accuracy: {ensemble_accuracy:.4f}')\nprint('Ensemble Classification Report:\\n', classification_report(y_val, ensemble_pred))\n\n# Make final predictions with ensemble\nprint(\"Making final predictions...\")\nfinal_predictions = ensemble.predict(X_test_scaled)\n\n# Create submission file\nsubmission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': final_predictions.astype(int)})\nsubmission.to_csv('submission.csv', index=False)\nprint('Submission file created.')\n\n# Summary of model performance\nprint(\"\\nModel Performance Summary:\")\nfor model_name, metrics in results.items():\n    print(f\"{model_name}: Accuracy = {metrics['accuracy']:.4f}, F1 = {metrics['f1']:.4f}\")\nprint(f\"Ensemble Model: Accuracy = {ensemble_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:02:23.673469Z","iopub.execute_input":"2025-03-22T06:02:23.673705Z","iopub.status.idle":"2025-03-22T06:02:28.638684Z","shell.execute_reply.started":"2025-03-22T06:02:23.673689Z","shell.execute_reply":"2025-03-22T06:02:28.637862Z"}},"outputs":[],"execution_count":null}]}